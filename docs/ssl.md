SSL certificates are issed by LetsEncrypt using the DNS-01 protocol.

Each leaf gets a cert that is valid for leafname.rippy.rip _and_ irc.rippy.rip.

The hub is in charge of issuing certificates and distributing them to the leaves.

The hub has a user 'ssl-controller'. This user has:

* An SSH key that is in the the authorized_keys file for the user 'ssl' on each leaf server
* The LetsEncrypt API key (generated by the initial setup Ansible playbook)
* The CloudFlare DNS API key (provided to the playbook by us)

The user will periodically (once a day?) run an Ansible policy that will:

* Check if the certs on any leaf are invalid or nearing expiry
* If so, issue a new one by changing DNS with CloudFlare and talking to LetsEncrypt
* Upload this cert to the leaf
* Reload the IRC server config on the leaf to pick up the new key

The leaves will have a user 'ssl' which will have no special permissions, but is able to:

* copy the SSL cert in place (it has ownership over the directory it resides in)
* reload the IRC server config (via a passwordless sudo entry for this specific command)

# Ansible policies

There are two:

* `./playbook.yml` - this performs initial setup, creating users, generating LE key, etc, that only needs to be done once. This is run from our machines and needs to be run on all the hub/leaf servers together by someone with root access to all of them.
* `./ssl-controller-policy/` - this is a multi-file policy that is uploaded to the hub (by the initial setup policy) and is the one that runs regularly to actually update the certificates.

# Files

The Lets Encrypt account key will be stored in

`/home/ssl-controller/account-key.pem`

The IRC certificate is:

`/etc/ssl/ircd/cert.pem`

The parent directory for this is owned by the user 'ssl' and the group 'irc', so the ssl user can upload certs and the IRCd can read them.

`secrets.yml`

Various items we don't want to store in Git are fed to the ssl-controller Ansible policy via this file.

# Running in development

First, run:

`vagrant provision --provision-with=ansible`

This runs an Ansible policy that doesn't actually do anything, but prompts Vagrant to create inventory files for Ansible that point at the virtual machines.

Then to run the initial setup policy:

`make`

Or if initial setup hasn't changed and you only want to upload ssl-controller-policy:

`make quick`

# Run options

Just do the thing:

```
ansible-playbook -u ssl -i hosts.ini issue-certificates.yml
```

Do the thing on just one of the leaves with `--limit`:

```
ansible-playbook -u ssl -i hosts.ini --limit eu issue-certificates.yml
```

Run in "blind update" mode - this mode doesn't assume Inspircd is running, so it won't make any checks against the IRCd, rehash it, or confirm deployment of the certificate. Useful if Inspircd won't start until it has a certificate!

```
ansible-playbook -u ssl -i hosts.ini -e 'blind_upload=true' issue-certificates.yml
# or:
BLIND_UPLOAD=true ansible-playbook -u ssl -i hosts.ini issue-certificates.yml
```

# Initial deployment

The first deployment is complicated by the fact we've moved the certificate location, and while creating this is done via Ansible, the inspircd configuration is still in CFEngine.

Here's the play-by-play for the initial deployment:

1. Run the Ansible initial setup policy which will create the users, dirs, etc
2. Run the issue certificates policy from the hub in blind update mode, this will put the certs in place
3. Run the CFEngine policy to change Inspircd's configuration files
4. Run the issue certificates policy normally, which will then rehash Inspircd configuration and SSL

It is important to not run #2 unless you are ready to immediately run #3, as it will leave Inspircd in a state where it would not successfully restart.

Even consider running #3 first maybe? TODO